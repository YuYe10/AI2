''' begin generated by SiderAI'''
import os
import pandas as pd
from datasets import Dataset, DatasetDict

os.chdir("/mnt/g/AI/AI2/2-dataset")
# 读取数据集
data = pd.read_parquet("data/data.parquet")
num_rows = len(data)
is_null_value = data.isnull().values.any()
num_categories = len(data["label"].unique())
num_information_in_each_category = data["label"].value_counts()
print('num_rows:', num_rows, 'is_null_value:', is_null_value, '\n')
print('num_categories:', num_categories, \
    'num_information_in_each_category:', \
        num_information_in_each_category, '\n')
# 按照类别进行排序
data_sorted = data.sort_values('label')

# 计算每个类别数据量
min_category_count = min(num_information_in_each_category)
data_grouped = data.groupby("label")
data_balanced = data_grouped.apply(lambda x: x.sample(min_category_count)).reset_index(drop=True)

# 按 90%-10% 比例拆分数据集
train_size = int(0.9 * min_category_count)
train_data = data_balanced.groupby("label").head(train_size)
test_data = data_balanced[~data_balanced.index.isin(train_data.index)]

# 保存训练集和测试集为 parquet 文件
os.makedirs("data", exist_ok=True)
train_data.to_parquet("data/train.parquet", index=False)
test_data.to_parquet("data/test.parquet", index=False)

# 使用第一节中分割的数据 train_data 和 test_data
train_dataset = Dataset.from_pandas(train_data)
test_dataset = Dataset.from_pandas(test_data)

# 定义特征
FEATURES = {
    "text": "string",
    "label": "int8"
}

# 定义数据集配置
BUILDER_CONFIG = DatasetDict({
    "default": {
        "version": "1.0.0",
        "train": train_dataset,
        "test": test_dataset,
        "features": FEATURES
    }
})
''' end generated by SiderAI'''
